{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c34c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from cleantext import clean\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "# Example command:\n",
    "# python identify_paragraphs_containing_keywords.py C:\\Users\\jasonjia\\Dropbox\\Projects\\conference_call\\output\\02_process_cc\\02.2_csv_20210101_20220617 C:\\Users\\jasonjia\\Dropbox\\Projects\\conference_call\\output\\03_identify_paragraphs_containing_keywords C:\\Users\\jasonjia\\Dropbox\\Projects\\conference_call\\code\\03_identify_paragraphs_containing_keywords\\reference_files\\keywords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bc3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse arguments\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Identify paragraphs containing keywords')\n",
    "    parser.add_argument('inputfolder', help=\"input folder containing the .csv files\", type=str)\n",
    "    parser.add_argument('outputfolder', help=\"output folder containing the paragraphs\", type=str)\n",
    "    parser.add_argument('keywords_filepath', help=\"filepath of the keywords.txt file\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    inputfolder = Path(args.inputfolder)\n",
    "    outputfolder = Path(args.outputfolder)\n",
    "    keywords_filepath = Path(args.keywords_filepath)\n",
    "\n",
    "print(\"Input folder:\", inputfolder)\n",
    "print(\"Output folder:\", outputfolder)\n",
    "print(\"Keywords filepath:\", keywords_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0814136",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfolder = Path(r\"C:\\Users\\jasonjia\\Dropbox\\Projects\\conference_call\\output\\02_process_cc\\02.2_csv_20210101_20220617\")\n",
    "outputfolder = Path(r\"C:\\Users\\jasonjia\\Dropbox\\Projects\\conference_call\\output\\03_identify_paragraphs_containing_keywords\")\n",
    "keywords_filepath = Path(r\"C:\\Users\\jasonjia\\Dropbox\\Projects\\conference_call\\code\\03_identify_paragraphs_containing_keywords\\reference_files\\keywords.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ce6bc",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e47e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s):\n",
    "    t = s.replace(\"\\\\n\", \" \")\n",
    "    t = t.replace(\"\\\\\", \"\")\n",
    "    if len(t) == 0:\n",
    "        return \"\"\n",
    "    if t[0] == '\"':\n",
    "        t = t[1:-1]\n",
    "    return clean(t, fix_unicode = True, no_line_breaks = True, lower = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e091f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_keywords_from_one_call(row, keywords, clean = True, lower = True):\n",
    "    # This is an imperfect split, sometimes it breaks in the middle of a sentence\n",
    "    call = str(row[\"Call\"])\n",
    "    paras_list = [t.replace(\"\\n\",\" \") for t in re.split(r\"\\n\\s*\\n+\", call) if len(t)>0 and not re.match(r\"^\\s*[0-9]+$\",t)]\n",
    "    found_keywords, found_in_paras = [], []\n",
    "\n",
    "    report_id = row[\"Report\"]\n",
    "    for para in paras_list:\n",
    "        for keyword in keywords:\n",
    "            if lower == True:\n",
    "                para_lower = para.lower()\n",
    "                keyword_lower = keyword.lower()\n",
    "            if clean == True:\n",
    "                para = clean_str(para)\n",
    "            if lower == False:\n",
    "                if keyword in para:\n",
    "                    found_keywords.append(keyword)\n",
    "                    found_in_paras.append(para)\n",
    "            else:\n",
    "                if keyword_lower in para_lower:\n",
    "                    found_keywords.append(keyword)\n",
    "                    found_in_paras.append(para)\n",
    "    return pd.DataFrame({\"Keyword\": found_keywords, \"Para\": found_in_paras, \"Report\": report_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac474da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c014a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNumbers(text):\n",
    "    digits = re.findall(r\"[(\\d.)]+\", text)\n",
    "    digits = [i for i in digits if i != \".\"]\n",
    "    return digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc15f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_n_prev_and_next(p1, p2, k, n = 5):\n",
    "    inverted_n = n * -1 ### Good for indexing purposes\n",
    "    start = p1.index(k)\n",
    "    end = start + len(k)\n",
    "    p1_start_chunk, p1_end_chunk = p1[:start], p1[end:]\n",
    "    p2_start_chunk, p2_end_chunk = p2[:start], p2[end:]\n",
    "    p1_preceding_n_words, p2_preceding_n_words = \" \".join(p1_start_chunk.split()[inverted_n:]), \" \".join(p2_start_chunk.split()[inverted_n:])\n",
    "    p1_succeding_n_words, p2_succeding_n_words = \" \".join(p1_end_chunk.split()[:n]), \" \".join(p2_end_chunk.split()[:n])\n",
    "    if (p1_preceding_n_words == p2_preceding_n_words) and (p1_succeding_n_words == p2_succeding_n_words):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86748084",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Para'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Para'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m unclean_data \u001b[38;5;241m=\u001b[39m unclean_data\u001b[38;5;241m.\u001b[39mmerge(df, on \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReport\u001b[39m\u001b[38;5;124m\"\u001b[39m, how \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m     14\u001b[0m unclean_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m file\n\u001b[1;32m---> 15\u001b[0m unclean_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHasNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43munclean_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPara\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(has_numbers)\n\u001b[0;32m     16\u001b[0m outputfilepath \u001b[38;5;241m=\u001b[39m Path(outputfolder \u001b[38;5;241m/\u001b[39m file)\n\u001b[0;32m     17\u001b[0m unclean_data\u001b[38;5;241m.\u001b[39mto_parquet(outputfilepath, compression \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Para'"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "keywords_df = pd.read_csv(keywords_filepath, sep = \"\\t\", header = None)\n",
    "keywords = keywords_df[0]\n",
    "\n",
    "for file in inputfolder.iterdir():    \n",
    "    df_csv = pd.read_csv(file)\n",
    "    df_combined = []\n",
    "    for _, row in df_csv.iterrows():\n",
    "        df = alt_keywords_from_one_call(row, keywords, clean = False, lower = True)\n",
    "        df_combined.append(df)\n",
    "    \n",
    "    unclean_data = pd.concat(df_combined).reset_index(drop = True)\n",
    "    unclean_data = unclean_data.merge(df, on = \"Report\", how = \"left\") \n",
    "    unclean_data[\"File\"] = file\n",
    "    unclean_data[\"HasNumber\"] = unclean_data[\"Para\"].apply(has_numbers)\n",
    "    outputfilepath = Path(outputfolder / file)\n",
    "    unclean_data.to_parquet(outputfilepath, compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db7b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
